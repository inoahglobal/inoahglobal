"""
Memory Store - ChromaDB vector database for the Exocortex.
Stores and retrieves memories using semantic search.
"""

import os
import sys
from pathlib import Path
from typing import Optional, List, Dict, Any

import chromadb
from chromadb.config import Settings

# Add parent to path for shared imports
sys.path.insert(0, str(Path(__file__).parent.parent))

from shared import get_logger
from shared.config_loader import get_config

logger = get_logger("memory")


class MemoryStore:
    """
    Vector database for storing and retrieving memories.
    Uses ChromaDB with Ollama embeddings.
    """
    
    # Collection names
    COLLECTION_PROJECT = "project_context"  # Project history and decisions
    COLLECTION_CONVERSATIONS = "conversations"  # Past conversations
    COLLECTION_IDENTITY = "identity"  # Personal facts and preferences
    
    def __init__(self, persist_dir: Optional[str] = None):
        """
        Initialize the memory store.
        
        Args:
            persist_dir: Directory to persist ChromaDB data
        """
        config = get_config()
        
        # Default persist directory
        if persist_dir is None:
            persist_dir = str(Path(__file__).parent / "chromadb_data")
        
        self.persist_dir = persist_dir
        
        # Initialize ChromaDB with persistence
        logger.info(f"Initializing ChromaDB at {persist_dir}")
        
        self.client = chromadb.PersistentClient(
            path=persist_dir,
            settings=Settings(
                anonymized_telemetry=False,
                allow_reset=True
            )
        )
        
        # Get or create collections
        self._init_collections()
        
        logger.info("Memory store initialized")
    
    def _init_collections(self):
        """Initialize all collections."""
        # Project context collection
        self.project_collection = self.client.get_or_create_collection(
            name=self.COLLECTION_PROJECT,
            metadata={"description": "Project history, decisions, and architecture"}
        )
        
        # Conversations collection
        self.conversations_collection = self.client.get_or_create_collection(
            name=self.COLLECTION_CONVERSATIONS,
            metadata={"description": "Past conversation history"}
        )
        
        # Identity collection
        self.identity_collection = self.client.get_or_create_collection(
            name=self.COLLECTION_IDENTITY,
            metadata={"description": "Personal facts and preferences"}
        )
    
    def add_memory(
        self,
        text: str,
        collection: str = COLLECTION_PROJECT,
        metadata: Optional[Dict[str, Any]] = None,
        doc_id: Optional[str] = None
    ) -> str:
        """
        Add a memory to the store.
        
        Args:
            text: The text content to store
            collection: Which collection to store in
            metadata: Optional metadata (source, timestamp, etc.)
            doc_id: Optional document ID (auto-generated if not provided)
            
        Returns:
            Document ID
        """
        import hashlib
        import time
        
        # Generate ID if not provided
        if doc_id is None:
            doc_id = hashlib.md5(f"{text[:100]}{time.time()}".encode()).hexdigest()[:16]
        
        # Default metadata
        if metadata is None:
            metadata = {}
        
        metadata["timestamp"] = metadata.get("timestamp", time.time())
        
        # Get the collection
        coll = self._get_collection(collection)
        
        # Add to collection
        coll.add(
            documents=[text],
            metadatas=[metadata],
            ids=[doc_id]
        )
        
        logger.debug(f"Added memory {doc_id} to {collection}")
        return doc_id
    
    def add_memories_batch(
        self,
        texts: List[str],
        collection: str = COLLECTION_PROJECT,
        metadatas: Optional[List[Dict[str, Any]]] = None,
        ids: Optional[List[str]] = None
    ) -> List[str]:
        """
        Add multiple memories in a batch.
        
        Args:
            texts: List of text content
            collection: Which collection to store in
            metadatas: Optional list of metadata dicts
            ids: Optional list of document IDs
            
        Returns:
            List of document IDs
        """
        import hashlib
        import time
        
        # Generate IDs if not provided
        if ids is None:
            ids = [
                hashlib.md5(f"{t[:100]}{time.time()}{i}".encode()).hexdigest()[:16]
                for i, t in enumerate(texts)
            ]
        
        # Default metadata
        if metadatas is None:
            metadatas = [{"timestamp": time.time()} for _ in texts]
        else:
            for m in metadatas:
                m["timestamp"] = m.get("timestamp", time.time())
        
        # Get the collection
        coll = self._get_collection(collection)
        
        # Add batch
        coll.add(
            documents=texts,
            metadatas=metadatas,
            ids=ids
        )
        
        logger.info(f"Added {len(texts)} memories to {collection}")
        return ids
    
    def query(
        self,
        query_text: str,
        collection: str = COLLECTION_PROJECT,
        n_results: int = 5,
        where: Optional[Dict] = None
    ) -> List[Dict[str, Any]]:
        """
        Query memories using semantic search.
        
        Args:
            query_text: The query string
            collection: Which collection to search
            n_results: Number of results to return
            where: Optional filter conditions
            
        Returns:
            List of results with text, metadata, and distance
        """
        coll = self._get_collection(collection)
        
        results = coll.query(
            query_texts=[query_text],
            n_results=n_results,
            where=where
        )
        
        # Format results
        formatted = []
        if results["documents"] and results["documents"][0]:
            for i, doc in enumerate(results["documents"][0]):
                formatted.append({
                    "text": doc,
                    "metadata": results["metadatas"][0][i] if results["metadatas"] else {},
                    "id": results["ids"][0][i] if results["ids"] else None,
                    "distance": results["distances"][0][i] if results["distances"] else None
                })
        
        return formatted
    
    def query_all_collections(
        self,
        query_text: str,
        n_results: int = 3
    ) -> Dict[str, List[Dict[str, Any]]]:
        """
        Query all collections and return combined results.
        
        Args:
            query_text: The query string
            n_results: Number of results per collection
            
        Returns:
            Dict mapping collection name to results
        """
        results = {}
        
        for collection in [self.COLLECTION_PROJECT, self.COLLECTION_CONVERSATIONS, self.COLLECTION_IDENTITY]:
            try:
                results[collection] = self.query(query_text, collection, n_results)
            except Exception as e:
                logger.warning(f"Failed to query {collection}: {e}")
                results[collection] = []
        
        return results
    
    def get_relevant_context(
        self,
        query_text: str,
        max_tokens: int = 2000,
        n_results: int = 5
    ) -> str:
        """
        Get relevant context for a query, formatted for LLM injection.
        
        Args:
            query_text: The query/question
            max_tokens: Approximate max tokens for context
            n_results: Number of results to retrieve
            
        Returns:
            Formatted context string
        """
        # Query project context primarily
        results = self.query(query_text, self.COLLECTION_PROJECT, n_results)
        
        if not results:
            return ""
        
        # Build context string
        context_parts = []
        char_count = 0
        max_chars = max_tokens * 4  # Rough estimate
        
        for r in results:
            text = r["text"]
            if char_count + len(text) > max_chars:
                break
            context_parts.append(text)
            char_count += len(text)
        
        if not context_parts:
            return ""
        
        return "\n\n---\n\n".join(context_parts)
    
    def _get_collection(self, name: str):
        """Get collection by name."""
        if name == self.COLLECTION_PROJECT:
            return self.project_collection
        elif name == self.COLLECTION_CONVERSATIONS:
            return self.conversations_collection
        elif name == self.COLLECTION_IDENTITY:
            return self.identity_collection
        else:
            raise ValueError(f"Unknown collection: {name}")
    
    def get_stats(self) -> Dict[str, int]:
        """Get count of documents in each collection."""
        return {
            "project_context": self.project_collection.count(),
            "conversations": self.conversations_collection.count(),
            "identity": self.identity_collection.count()
        }
    
    def clear_collection(self, collection: str):
        """Clear all documents from a collection."""
        coll = self._get_collection(collection)
        
        # Get all IDs
        results = coll.get()
        if results["ids"]:
            coll.delete(ids=results["ids"])
            logger.info(f"Cleared {len(results['ids'])} documents from {collection}")


# Singleton instance
_memory_store: Optional[MemoryStore] = None


def get_memory_store() -> MemoryStore:
    """Get the singleton MemoryStore instance."""
    global _memory_store
    if _memory_store is None:
        _memory_store = MemoryStore()
    return _memory_store


